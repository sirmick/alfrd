<div align="center">
  <img src="ALFRD.svg" alt="ALFRD Logo" width="200" />
  
  # ALFRD - Automated Ledger & Filing Research Database
  
  > Your personal AI-powered document management system that ingests, processes, and summarizes documents automatically using AWS Textract OCR and LLM classification via AWS Bedrock.
</div>

## What is ALFRD?

**ALFRD** (Automated Ledger & Filing Research Database) is a personal document management system that uses AI to automatically process, categorize, and summarize your documents. Drop a document folder in the inbox and ALFRD will:

- **Extract text** using AWS Textract OCR with block-level data preservation
- **Process folders** with multiple documents (multi-page bills, receipts, etc.)
- **Classify via MCP** using LLM-powered document type detection (AWS Bedrock)
- **Extract structured data** (vendor, amount, due date, line items) from bills automatically
- **Organize into series** with schema-consistent extraction across recurring documents
- **Store in PostgreSQL** with full-text search capability
- **Preserve for LLMs** with combined text + block-level structure for spatial reasoning

## Quick Start

### Prerequisites
- Python 3.11+
- PostgreSQL 15+
- AWS credentials (for Textract OCR and Bedrock LLM)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/alfrd.git
cd alfrd

# Install dependencies
pip install -r requirements.txt

# Configure AWS credentials in .env
cp .env.example .env
# Edit .env and add AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION

# Initialize database (REQUIRED!)
./scripts/create-alfrd-db
```

### Process Your First Document

```bash
# 1. Add a document (creates folder with meta.json)
python scripts/add-document.py ~/Downloads/bill.jpg --tags bill utilities

# 2. Process documents (OCR + storage)
python3 document-processor/src/document_processor/main.py

# 3. Check results
ls -la data/processed/     # Processed folders
ls -la data/documents/     # Stored documents with extracted text
```

## Architecture Overview

### Folder-Based Document Input

Documents are organized in folders with metadata:

```
data/inbox/
â””â”€â”€ bill_20241125_120000/
    â”œâ”€â”€ meta.json          # Document metadata
    â”œâ”€â”€ bill.jpg           # Page 1
    â””â”€â”€ page2.jpg          # Page 2
```

### Asyncio Processing Pipeline with Recovery

```
User adds document â†’ Folder created in inbox (PENDING)
                     â†“
         OCR Step â†’ AWS Textract OCR (OCR_COMPLETED)
                     â†“
    Classify Step â†’ Bedrock LLM classification (CLASSIFIED)
                     â†“
                Background: Score Classification (for prompt evolution)
                     â†“
   Summarize Step â†’ Type-specific summarization (SUMMARIZED)
                     â†“
                Background: Score Summary (for prompt evolution)
                     â†“
       File Step â†’ Series detection & tagging (FILED)
                     â†“
Series Summarize â†’ Series-specific extraction (SERIES_SUMMARIZED)
                     â†“
                Background: Score Series Extraction
                     â†“
         Complete â†’ Updates status (COMPLETED)
```

**Processing Features:**
- Asyncio orchestrator with semaphore-based concurrency control
- Automatic retry on failure (max 3 attempts per document)
- Periodic recovery scan for stuck work (every 5 minutes)
- 30-minute timeout for stale work detection
- Scoring steps run in background (fire-and-forget)
- PostgreSQL advisory locks prevent race conditions
- Comprehensive event logging for debugging

**Series-Specific Extraction (Schema Consistency):**
- Each document series gets its own extraction prompt
- All documents in a series have identical field names
- Eliminates schema drift (e.g., `total_amount` vs `amount_due`)
- Enables clean data tables and aggregation
- Prompts evolve together (all documents regenerated)

**Self-Improving Features:**
- Classifier prompt evolution based on classification accuracy
- Summarizer prompt evolution based on extraction quality
- Series prompt evolution with automatic regeneration
- LLM can suggest new document types
- Configure via `PROMPT_UPDATE_THRESHOLD` in .env (default: 0.05)

### LLM-Optimized Output

```json
{
  "full_text": "--- Document: bill.jpg ---\n[extracted text]\n\n--- Document: page2.jpg ---\n[more text]",
  "blocks_by_document": [
    {
      "file": "bill.jpg",
      "blocks": {
        "PAGE": [...],
        "LINE": [...],
        "WORD": [...]
      }
    }
  ],
  "document_count": 2,
  "total_chars": 1234,
  "avg_confidence": 0.95
}
```

## Key Features

### ğŸ§  AI-Powered Processing
- **ğŸ¤– Self-improving prompts** - Classifier, summarizer, and series prompts evolve based on quality
- **ğŸ”® Dynamic classification** - LLM can suggest new document types automatically
- **ğŸ“ Type-specific summarization** - DB-driven, customizable per document type
- **ğŸ¯ Series-specific extraction** - Each document series gets consistent field names
- **âš¡ Nova Lite inference** - Fast, cost-effective AWS Bedrock processing

### ğŸ“„ Document Processing
- **ğŸ‘ï¸ AWS Textract OCR** - 95%+ accuracy with block-level data preservation
- **ğŸ“¦ Multi-page support** - Process multiple images as single document
- **ğŸ·ï¸ Flexible tagging** - Secondary tags for rich classification
- **ğŸ“‚ Series detection** - Automatic grouping of recurring documents
- **â™»ï¸ Series regeneration** - All documents updated when series prompt improves

### ğŸ”§ Robust Architecture
- **âš™ï¸ Asyncio orchestration** - Semaphore-based concurrency control
- **ğŸ”’ PostgreSQL advisory locks** - Prevent race conditions in concurrent processing
- **ğŸ”„ Automatic recovery** - Retry on failure (max 3 attempts) + stale work detection
- **ğŸ“Š Event logging system** - Comprehensive debugging with `./scripts/view-events`
- **ğŸ“ˆ Prompt versioning** - All prompt changes tracked with version history

### ğŸ“± Web Interface (PWA)
- **ğŸ“· Camera capture** - Take photos directly from mobile devices
- **ğŸ“‹ Document list** - Real-time data from API with refresh
- **ğŸ” Document detail** - Full metadata, OCR text, structured data display
- **ğŸ“Š Data tables** - Flattened JSONB visualization

### ğŸ—„ï¸ Data & Storage
- **ğŸ˜ PostgreSQL** - Full-text search and structured JSONB data
- **ğŸ“ JSON flattening** - Convert nested data to flat tables with 4 array strategies
- **ğŸ“¤ CSV export** - CLI tool for data extraction and analysis
- **ğŸ”— LLM-optimized format** - Combined text + block-level structure for spatial reasoning

### âœ… Testing & Quality
- **ğŸ§ª 113+ tests** - Comprehensive pytest suite
- **ğŸ”¬ Workflow tests** - End-to-end scenarios with real data
- **ğŸ›¡ï¸ Database tests** - Direct PostgreSQL operation validation
- **ğŸ“Š API tests** - FastAPI endpoint coverage

## Project Structure

```
alfrd/
â”œâ”€â”€ document-processor/        # Asyncio orchestrator
â”‚   â”œâ”€â”€ src/document_processor/
â”‚   â”‚   â”œâ”€â”€ main.py           # Entry point
â”‚   â”‚   â”œâ”€â”€ orchestrator.py   # SimpleOrchestrator with recovery
â”‚   â”‚   â”œâ”€â”€ tasks/
â”‚   â”‚   â”‚   â”œâ”€â”€ document_tasks.py     # Processing steps
â”‚   â”‚   â”‚   â””â”€â”€ series_regeneration.py # Series regeneration worker
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ locks.py      # PostgreSQL advisory locks
â”‚   â”‚   â””â”€â”€ extractors/
â”‚   â”‚       â””â”€â”€ aws_textract.py    # AWS Textract OCR with blocks
â”‚   â””â”€â”€ tests/                # Pytest test suite
â”œâ”€â”€ api-server/               # REST API (30+ endpoints)
â”œâ”€â”€ mcp-server/               # LLM tools (library functions)
â”‚   â””â”€â”€ src/mcp_server/tools/
â”‚       â”œâ”€â”€ summarize_series.py  # Series-specific extraction
â”‚       â””â”€â”€ ...
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ create-alfrd-db      # Database initialization (REQUIRED!)
â”‚   â”œâ”€â”€ add-document         # Add documents to inbox
â”‚   â”œâ”€â”€ view-events          # Event log viewer
â”‚   â””â”€â”€ start-processor      # Process documents wrapper
â”œâ”€â”€ shared/                   # Shared configuration and types
â”‚   â”œâ”€â”€ database.py          # PostgreSQL client
â”‚   â”œâ”€â”€ event_logger.py      # Event logging utilities
â”‚   â””â”€â”€ tests/
â””â”€â”€ data/                    # Runtime data (not in git)
    â”œâ”€â”€ inbox/              # Document folders (input)
    â”œâ”€â”€ processed/          # Processed folders (archived)
    â”œâ”€â”€ documents/          # Stored documents (output)
    â””â”€â”€ postgres/           # PostgreSQL data (Docker)
```

## Usage Examples

### Add Documents

```bash
# Single image
python scripts/add-document.py photo.jpg --tags receipt

# Multiple pages
python scripts/add-document.py page1.jpg page2.jpg page3.jpg --tags bill electric

# With source
python scripts/add-document.py doc.jpg --source mobile --tags insurance
```

### Process Documents

```bash
# Process all documents in inbox (continuous mode)
python3 document-processor/src/document_processor/main.py

# Run once and exit
python3 document-processor/src/document_processor/main.py --once

# Process single document
python3 document-processor/src/document_processor/main.py --doc-id <UUID>

# Or use wrapper script
./scripts/start-processor
```

### Test OCR

```bash
# See detailed Textract block output
python samples/test_ocr.py samples/pg\&e-bill.jpg
```

### Run Tests

```bash
# Install pytest
pip install pytest pytest-asyncio

# Run storage tests
pytest document-processor/tests/test_storage.py -v
```

## meta.json Format

Each document folder requires a `meta.json` file:

```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "created_at": "2024-11-25T02:00:00Z",
  "documents": [
    {"file": "bill.jpg", "type": "image", "order": 1},
    {"file": "page2.jpg", "type": "image", "order": 2}
  ],
  "metadata": {
    "source": "mobile",
    "tags": ["bill", "utilities"]
  }
}
```

The `add-document.py` script creates this automatically.

## Configuration

### Environment Variables

```bash
# AWS Credentials (Required)
AWS_ACCESS_KEY_ID=your-access-key
AWS_SECRET_ACCESS_KEY=your-secret-key
AWS_REGION=us-east-1

# Paths (Optional - defaults for local development)
DATABASE_URL=postgresql://alfrd_user@/alfrd?host=/var/run/postgresql
POSTGRES_PASSWORD=alfrd_dev_password
INBOX_PATH=./data/inbox
DOCUMENTS_PATH=./data/documents
SUMMARIES_PATH=./data/summaries

# Logging
LOG_LEVEL=INFO
ENV=development
```

## Common Issues

### Database not initialized

```bash
# Error: relation "documents" does not exist
# Solution: Initialize database
./scripts/create-alfrd-db
```

### AWS credentials not configured

```bash
# Error: AWS authentication failed
# Solution: Set up credentials in .env
AWS_ACCESS_KEY_ID=your-key
AWS_SECRET_ACCESS_KEY=your-secret
AWS_REGION=us-east-1
```

## Development

### Running Tests

```bash
# Run all tests
pytest -v

# Run specific test file
pytest document-processor/tests/test_storage.py -v

# Run with coverage
pytest --cov=document_processor
```

### Code Structure

All main scripts have built-in PYTHONPATH setup for standalone execution:

```python
# At top of file
_script_dir = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(_script_dir))  # Project root
sys.path.insert(0, str(Path(__file__).parent.parent))  # src directory
```

No wrapper scripts or environment setup needed!

## Technical Details

### Technologies

- **Python 3.11+** - Core language
- **AWS Textract** - Production OCR ($1.50/1000 pages)
- **AWS Bedrock** - LLM for classification/summarization (Nova Lite)
- **PostgreSQL 15+** - Production database with full-text search
- **FastAPI** - REST API framework
- **Asyncio** - Orchestration and concurrency
- **Pytest** - Testing framework

### Database Schema

Key tables:
- `documents` - Core document metadata, extracted text, and structured data
- `series` - Document series with active_prompt_id and regeneration_pending
- `prompts` - Evolving classifier, summarizer, and series prompts (versioned)
- `events` - Comprehensive event log for debugging
- `document_types` - Dynamic list of known document types
- `classification_suggestions` - LLM-suggested new document types

See `api-server/src/api_server/db/schema.sql` for complete schema.

## Statistics

- **ğŸ“ Lines of Code**: ~8,000+ lines (orchestrator + tasks + MCP tools + API + Web UI + events)
- **ğŸ§ª Test Coverage**: 113+ tests passing (database, API, workflow, locks)
- **ğŸ‘ï¸ OCR Accuracy**: 95%+ with AWS Textract
- **âš¡ Processing Speed**: ~2-3 seconds per page
- **âš™ï¸ Orchestration**: Simple asyncio with semaphore-based concurrency control
- **ğŸ”„ Recovery**: Automatic retry (3 attempts) + periodic stale work detection (5 min)
- **ğŸ¤– LLM Integration**: AWS Bedrock with Amazon Nova Lite
- **ğŸ“ˆ Prompt Evolution**: Enabled with threshold=0.05 (configure via PROMPT_UPDATE_THRESHOLD)
- **ğŸ“‚ Series Prompts**: One per series for schema-consistent extraction
- **ğŸ·ï¸ Document Types**: 6 default types + unlimited LLM-suggested types
- **ğŸ”— API Endpoints**: 30+ endpoints (health, documents, files, series, tags, prompts, events)
- **ğŸ“Š Event Logging**: Full audit trail with `./scripts/view-events`
- **ğŸ“± Web UI**: Ionic React PWA with data visualization
- **ğŸ“ Data Analysis**: JSON flattening utility with 4 array strategies and pandas integration

## Contributing

See `STATUS.md` for current status and development roadmap.

## License

MIT License - see `LICENSE` file for details.

## Documentation

- **`START_HERE.md`** - Quick start guide
- **`ARCHITECTURE.md`** - System architecture and design decisions
- **`STATUS.md`** - Current status and development roadmap
- **`docs/JSON_FLATTENING.md`** - JSON flattening utilities guide

---

**ğŸš€ Process your documents with AI-powered OCR and classification!**
